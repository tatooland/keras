{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "word2vec-keras.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tatooland/keras/blob/master/word2vec_keras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zOyaaXOFDDr_",
        "colab_type": "code",
        "outputId": "52784859-8d4b-4763-d57f-162ff3439411",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!pip install -U -q PyDrive\n",
        "!pip install jieba"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: jieba in /usr/local/lib/python3.6/dist-packages (0.39)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FWcrJKEVCAtN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "from zipfile import ZipFile\n",
        "from shutil import copy\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "import os\n",
        "from zipfile import ZipFile\n",
        "\n",
        "def downloadFromGDriver(fileId):\n",
        "  auth.authenticate_user()\n",
        "  gauth = GoogleAuth()\n",
        "  gauth.credentials = GoogleCredentials.get_application_default()\n",
        "  drive = GoogleDrive(gauth)\n",
        "  fileName = fileId + '.zip'\n",
        "  downloaded = drive.CreateFile({'id': fileId})\n",
        "  downloaded.GetContentFile(fileName)\n",
        "  ds = ZipFile(fileName)\n",
        "  ds.extractall()\n",
        "  os.remove(fileName)\n",
        "  print('Extracted zip file ' + fileName)                  \n",
        "  \n",
        "  \n",
        "  \n",
        "def uploadToDriver(file):\n",
        "  auth.authenticate_user()\n",
        "  gauth = GoogleAuth()\n",
        "  gauth.credentials = GoogleCredentials.get_application_default()\n",
        "  drive = GoogleDrive(gauth)\n",
        "\n",
        "  # Create & upload a file.\n",
        "  uploaded = drive.CreateFile({'title': file})\n",
        "  uploaded.SetContentFile(file)\n",
        "  uploaded.Upload()\n",
        "  print('Uploaded file with ID {}'.format(uploaded.get('id')))\n",
        "\n",
        "\n",
        "def uncompressZipFile(fileName):\n",
        "   ds = ZipFile(fileName)\n",
        "   ds.extractall()\n",
        "   os.remove(fileName)\n",
        "   print(\"process complete\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gd9a3zWEGqqp",
        "colab_type": "code",
        "outputId": "40a6c2ce-54b8-45ef-d271-7cecfc38da68",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        }
      },
      "source": [
        "fileId = '1GRHNEZIct61cEOzeo1MHa7EBZSi7cFuy'\n",
        "downloadFromGDriver(fileId)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0828 10:00:39.583370 140231834793856 __init__.py:44] file_cache is unavailable when using oauth2client >= 4.0.0 or google-auth\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/googleapiclient/discovery_cache/__init__.py\", line 36, in autodetect\n",
            "    from google.appengine.api import memcache\n",
            "ModuleNotFoundError: No module named 'google.appengine'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/googleapiclient/discovery_cache/file_cache.py\", line 33, in <module>\n",
            "    from oauth2client.contrib.locked_file import LockedFile\n",
            "ModuleNotFoundError: No module named 'oauth2client.contrib.locked_file'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/googleapiclient/discovery_cache/file_cache.py\", line 37, in <module>\n",
            "    from oauth2client.locked_file import LockedFile\n",
            "ModuleNotFoundError: No module named 'oauth2client.locked_file'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/googleapiclient/discovery_cache/__init__.py\", line 41, in autodetect\n",
            "    from . import file_cache\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/googleapiclient/discovery_cache/file_cache.py\", line 41, in <module>\n",
            "    'file_cache is unavailable when using oauth2client >= 4.0.0 or google-auth')\n",
            "ImportError: file_cache is unavailable when using oauth2client >= 4.0.0 or google-auth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracted zip file 1GRHNEZIct61cEOzeo1MHa7EBZSi7cFuy.zip\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I-z4U0vm3XyT",
        "colab_type": "code",
        "outputId": "f84e5d27-ee76-4e4a-c77a-8654f91f7c3d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "adc.json  \u001b[0m\u001b[01;34m__MACOSX\u001b[0m/  \u001b[01;34msample_data\u001b[0m/  wunianwenji.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WaC9gcPZMJ99",
        "colab_type": "code",
        "outputId": "df95de65-42ad-4f06-8e7c-0ef8f70dd46e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        }
      },
      "source": [
        "import jieba\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import *\n",
        "from tensorflow.keras.preprocessing.sequence import skipgrams\n",
        "\n",
        "with open(\"./wunianwenji.txt\", 'r+', encoding='utf-8') as f:\n",
        "  contents = f.read()\n",
        "  splitList = jieba.cut(contents, cut_all=False)\n",
        "  splitList = list(splitList)\n",
        "  text = \" \".join(splitList)\n",
        "  \n",
        "  tokenizer = Tokenizer()\n",
        "  tokenizer.fit_on_texts([text])\n",
        "  \n",
        "  word2id = tokenizer.word_index\n",
        "  id2word = {v: k for k, v in word2id.items()}\n",
        "  \n",
        "  \n",
        "  wids = [word2id[w] for w in text_to_word_sequence(text)]\n",
        "  #pairs, labels = skipgrams(wids, len(word2id))\n",
        "  vocab_size = len(word2id) + 1\n",
        "  embed_size = 300\n",
        "  pairs, labels = skipgrams(wids, len(word2id))\n",
        "  '''\n",
        "  for i in range(10):\n",
        "    print(\"({:s} ({:d}), {:s} ({:d})) -> {:d}\".format(id2word[pairs[i][0]], pairs[i][0], id2word[pairs[i][1]], pairs[i][1], labels[i]))\n",
        "  '''\n",
        "    \n",
        "  print(type(pairs), type(labels))\n",
        "  \n",
        "  \n",
        "  \n",
        "  '''\n",
        "  charList = set(splitList)\n",
        "  spliteListSize = len(splitList)\n",
        "  charListSize = len(charList)\n",
        "  char2Index = dict((c, i) for i, c in enumerate(charList))\n",
        "  index2Char = dict((i, c) for i, c in enumerate(charList))\n",
        "  i = 0 \n",
        "  inputCode = []\n",
        "  charList1H = np.zeros((len(splitList), len(charList)), dtype=np.bool)\n",
        "  for idx, token in enumerate(splitList):\n",
        "    #inputCode.append(char2Index[token]) \n",
        "    charList1H[i][char2Index[token]] = 1  \n",
        "    \n",
        "    if i < 10:\n",
        "      print(char2Index[token], i)\n",
        "    i = i + 1\n",
        "  print(charList1H[0][9714])\n",
        "  '''\n",
        "  i = 0\n",
        "  for ele, l in zip(pairs, labels):\n",
        "    i = i + 1\n",
        "    print(ele, l)\n",
        "    if i == 10:\n",
        "      break\n",
        "  "
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Building prefix dict from the default dictionary ...\n",
            "I0828 10:00:44.952992 140231834793856 __init__.py:111] Building prefix dict from the default dictionary ...\n",
            "Loading model from cache /tmp/jieba.cache\n",
            "I0828 10:00:44.955661 140231834793856 __init__.py:131] Loading model from cache /tmp/jieba.cache\n",
            "Loading model cost 0.751 seconds.\n",
            "I0828 10:00:45.706606 140231834793856 __init__.py:163] Loading model cost 0.751 seconds.\n",
            "Prefix dict has been built succesfully.\n",
            "I0828 10:00:45.709352 140231834793856 __init__.py:164] Prefix dict has been built succesfully.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "<class 'list'> <class 'list'>\n",
            "[6, 896] 1\n",
            "[4400, 4579] 1\n",
            "[605, 1] 1\n",
            "[301, 727] 0\n",
            "[260, 22130] 0\n",
            "[13, 11] 1\n",
            "[1, 4393] 0\n",
            "[30, 24164] 0\n",
            "[16914, 9600] 1\n",
            "[93, 1] 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v27_7CDmqMVx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SimilarityCallback:\n",
        "    def run_sim(self):\n",
        "        for i in range(valid_size):\n",
        "            valid_word = reverse_dictionary[valid_examples[i]]\n",
        "            top_k = 8  # number of nearest neighbors\n",
        "            sim = self._get_sim(valid_examples[i])\n",
        "            nearest = (-sim).argsort()[1:top_k + 1]\n",
        "            log_str = 'Nearest to %s:' % valid_word\n",
        "            for k in range(top_k):\n",
        "                close_word = reverse_dictionary[nearest[k]]\n",
        "                log_str = '%s %s,' % (log_str, close_word)\n",
        "            print(log_str)\n",
        "\n",
        "    @staticmethod\n",
        "    def _get_sim(valid_word_idx):\n",
        "        sim = np.zeros((vocab_size,))\n",
        "        in_arr1 = np.zeros((1,))\n",
        "        in_arr2 = np.zeros((1,))\n",
        "        for i in range(vocab_size):\n",
        "            in_arr1[0,] = valid_word_idx\n",
        "            in_arr2[0,] = i\n",
        "            out = validation_model.predict_on_batch([in_arr1, in_arr2])\n",
        "            sim[i] = out\n",
        "        return sim\n",
        "sim_cb = SimilarityCallback()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kqrx0IOXliby",
        "colab_type": "code",
        "outputId": "158df846-f966-4371-84bb-c4a2b2652b86",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 987
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import dot\n",
        "from tensorflow.keras.layers import Dense, Reshape, Embedding\n",
        "#from tensorflow.keras.layers.embeddings import Embedding\n",
        "from tensorflow.keras.models import Sequential\n",
        "from IPython.display import SVG\n",
        "from tensorflow.keras.utils import plot_model\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "word_model_input = tf.keras.Input((1, ))\n",
        "word_model_emb = Embedding(vocab_size, embed_size, embeddings_initializer='glorot_uniform', input_length=1)(word_model_input)\n",
        "word_model = Reshape((embed_size, 1))(word_model_emb)\n",
        "\n",
        "context_model_input = tf.keras.Input((1, ))\n",
        "context_model_emb = Embedding(vocab_size, embed_size, embeddings_initializer='glorot_uniform', input_length=1)(context_model_input)\n",
        "context_model = Reshape((embed_size, 1))(context_model_emb)\n",
        "\n",
        "dot_product = dot([word_model, context_model], axes=1)\n",
        "dot_product = Reshape((1,))(dot_product)\n",
        "\n",
        "z = Dense(1, kernel_initializer='glorot_uniform', activation='sigmoid')(dot_product)\n",
        "model = tf.keras.Model(inputs=[word_model_input, context_model_input], outputs=z)\n",
        "model.compile(loss='mean_squared_error', optimizer='rmsprop')#mean_squared_error\n",
        "\n",
        "print(model.summary())\n",
        "\n",
        "\n",
        "#SVG(plot_model(model, show_shapes=True, show_layer_names=True))\n",
        "\n",
        "\n",
        "word_target, word_context = zip(*pairs)\n",
        "word_target = np.array(word_target, dtype=\"int32\")\n",
        "word_context = np.array(word_context, dtype=\"int32\")\n",
        "\n",
        "\n",
        "arr_1 = np.zeros((1,))\n",
        "arr_2 = np.zeros((1,))\n",
        "arr_3 = np.zeros((1,))\n",
        "for epoch in range(1, 10):\n",
        "  loss = 0\n",
        "  for i in range(1, 15000):\n",
        "    idx = np.random.randint(0, len(labels)-1)\n",
        "    arr_1[0,] = word_target[idx]\n",
        "    arr_2[0,] = word_context[idx]\n",
        "    arr_3[0,] =labels[idx]\n",
        "    loss = model.train_on_batch([arr_1, arr_2], arr_3)\n",
        "  \n",
        "    if i % 1000 == 0:\n",
        "      print('epoch {}, Iteration {}, loss={}'.format(epoch, i, loss))\n",
        "    if i % 5000 == 0:\n",
        "      sim_cb.run_sim()\n",
        "\n",
        "print('loss: {}'.format(loss))\n",
        "  \n",
        "'''\n",
        "  for i, elem in enumerate(pairs):\n",
        "    pair_first_elem = np.array(list(zip(*elem[0]))[0], dtype='int32')\n",
        "    pair_second_elem = np.array(list(zip(*elem[0]))[1], dtype='int32')\n",
        "    Y = np.array(labels, dtype='int32')\n",
        "    X = [pair_first_elem, pair_second_elem]\n",
        "    if i % 10000 == 0:\n",
        "      print('Processed {} (skip_first, skip_second, relevance) pairs'.format(i))\n",
        "    loss += model.train_on_batch(X, Y)\n",
        "  \n",
        "  print('Epoch:', epoch, 'Loss:', loss)\n",
        "'''"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0828 10:01:27.570672 140231834793856 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_2 (InputLayer)            [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_3 (InputLayer)            [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding (Embedding)           (None, 1, 300)       7711800     input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "embedding_1 (Embedding)         (None, 1, 300)       7711800     input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "reshape (Reshape)               (None, 300, 1)       0           embedding[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "reshape_1 (Reshape)             (None, 300, 1)       0           embedding_1[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dot (Dot)                       (None, 1, 1)         0           reshape[0][0]                    \n",
            "                                                                 reshape_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "reshape_2 (Reshape)             (None, 1)            0           dot[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 1)            2           reshape_2[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 15,423,602\n",
            "Trainable params: 15,423,602\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "epoch 1, Iteration 1000, loss=0.25587934255599976\n",
            "epoch 1, Iteration 2000, loss=0.25646039843559265\n",
            "epoch 1, Iteration 3000, loss=0.14094965159893036\n",
            "epoch 1, Iteration 4000, loss=0.25766873359680176\n",
            "epoch 1, Iteration 5000, loss=0.24389632046222687\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/api/_v1/keras/preprocessing/text/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'epoch {}, Iteration {}, loss={}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m5000\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m       \u001b[0msim_cb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_sim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'loss: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/api/_v1/keras/preprocessing/text/__init__.py\u001b[0m in \u001b[0;36mrun_sim\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mSimilarityCallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun_sim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m             \u001b[0mvalid_word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreverse_dictionary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvalid_examples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m             \u001b[0mtop_k\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m8\u001b[0m  \u001b[0;31m# number of nearest neighbors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'valid_size' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oAien9wjM2fk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_target, word_context = zip(*pairs)\n",
        "word_target = np.array(word_target, dtype=\"int32\")\n",
        "word_context = np.array(word_context, dtype=\"int32\")\n",
        "\n",
        "print()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}